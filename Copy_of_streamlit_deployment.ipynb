{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of streamlit deployment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46i2YHdouLLD"
      },
      "source": [
        "# Installing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hj6Fu5dz-xa1"
      },
      "source": [
        "!pip install streamlit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XO3yQr-RxcSi"
      },
      "source": [
        "!pip install pyngrok"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQ1EePjSuTgZ"
      },
      "source": [
        "# Importing Libraries and Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3S_AHYv0tZJ3"
      },
      "source": [
        "import streamlit as st  ## streamlit\n",
        "import pandas as pd  ## for data manipulation\n",
        "import pickle   ## For model loading \n",
        "import spacy  ## For NLP tasks \n",
        "import time\n",
        "\n",
        "from PIL import Image   ## For image\n",
        "from io import StringIO  ## for text input and output from the web app"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f35Irkfu-2t2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f4e838b-fdb8-493b-fa31-80e145ed5945"
      },
      "source": [
        "from keras.models import load_model\n",
        "# load model\n",
        "model = load_model('/content/my_model.h5')\n",
        "# summarize model.\n",
        "model.summary()\n",
        "# load dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_36 (Dense)             (None, 512)               51712     \n",
            "_________________________________________________________________\n",
            "dense_37 (Dense)             (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dense_38 (Dense)             (None, 6)                 3078      \n",
            "=================================================================\n",
            "Total params: 317,446\n",
            "Trainable params: 317,446\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RHv8ImVtp6A",
        "outputId": "d898da81-5b2f-4137-fc5e-e43dbf28c28e"
      },
      "source": [
        "# load the model from disk\n",
        "# Load model from file\n",
        "from sklearn.externals import joblib\n",
        "classifier = joblib.load(\"/content/pickle_nb.pkl\")\n",
        "classifier"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdsXpbx7ucZQ"
      },
      "source": [
        "# Streamlit App"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5JmExybuOFW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e922d87-cc94-46bf-c85f-2c678dda9eee"
      },
      "source": [
        "# defining the function which will make the prediction using the data which the user inputs\n",
        "%%writefile app.py \n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.externals import joblib\n",
        "\n",
        "def welcome(): \n",
        "  return 'welcome all'\n",
        "\n",
        "# defining the function which will make the prediction using  \n",
        "# the data which the user inputs \n",
        "def prediction(tweet):\n",
        "  from keras.models import load_model\n",
        "  model = load_model('/content/sent_model.h5')\n",
        "  classifier = joblib.load(\"/content/pickle_nb.pkl\")\n",
        "  # Making predictions \n",
        "  \n",
        "  from keras.preprocessing.text import Tokenizer\n",
        "  from keras.preprocessing.sequence import pad_sequences\n",
        "  from sklearn.preprocessing import LabelEncoder\n",
        "  import numpy as np\n",
        "  #label_encoder = LabelEncoder()\n",
        "  text=np.array([tweet])\n",
        "  tk=Tokenizer(num_words=4000,lower=True,split=\" \")\n",
        "  tk.fit_on_texts(text)\n",
        "  prediction = model.predict_classes(pad_sequences(tk.texts_to_sequences(text),\n",
        "  maxlen=100))\n",
        "  if prediction == 0:\n",
        "    prediction = 'negative'\n",
        "    return prediction\n",
        "  elif prediction == 1:\n",
        "    prediction = 'neutral'\n",
        "    return prediction\n",
        "  else:\n",
        "    prediction = 'positive'\n",
        "    return prediction\n",
        "  text=np.array([tweet])\n",
        "  tk=Tokenizer(num_words=4000,lower=True,split=\" \")\n",
        "  tk.fit_on_texts(text)\n",
        "  prediction2 = classifier.predict_classes(pad_sequences(tk.texts_to_sequences(text),\n",
        "  maxlen=100))\n",
        "  if prediction2 == 0:\n",
        "    prediction2 = 'negative'\n",
        "    return prediction2\n",
        "  elif prediction2 == 1:\n",
        "    prediction2 = 'neutral'\n",
        "    return prediction2\n",
        "  else:\n",
        "    prediction2 = 'positive'\n",
        "    return prediction2\n",
        "# this is the main function in which we define our webpage \n",
        "def main():\n",
        "  import streamlit as st\n",
        "   # giving the webpage a title \n",
        "  st.title(\"Tweet Prediction\") \n",
        "  st.set_option('deprecation.showPyplotGlobalUse', False)\n",
        "  st.sidebar.title(\"Info\")\n",
        "\n",
        "  st.markdown(\"## Input Tweet in English\")\n",
        "  st.sidebar.markdown(\"This app uses artificial neural networks to predict the sentiment of a tweet \")\n",
        "\n",
        "    # front end elements of the web page \n",
        "  html_temp = \"\"\" \n",
        "  <div style =\"background-color:purple;padding:13px\"> \n",
        "  <h1 style =\"color:black;text-align:center;\">Streamlit Eagles Tweet App</h1> \n",
        "  </div> \n",
        "    \"\"\"\n",
        "    # display the front end aspect\n",
        "  st.markdown(html_temp, unsafe_allow_html = True) \n",
        "    # following lines create boxes in which user can enter data required to make prediction \n",
        "  Tweet = st.text_input(\"Tweet\")\n",
        "  result =\"\"\n",
        "        \n",
        "      # when 'Predict' is clicked, make the prediction and store it \n",
        "  if st.button(\"Predict\"): \n",
        "    result = prediction(Tweet) \n",
        "  st.success('The sentiment of the tweet is {}'.format(result))\n",
        "  # else:\n",
        "  #   result = prediction2(Tweet) \n",
        "  # st.success('The aspect of the tweet is {}'.format(result))\n",
        "\n",
        "data_path = (\"/content/clean_health_data.csv\")\n",
        "\n",
        "@st.cache()\n",
        "def load_data():\n",
        "    data = pd.read_csv(data_path)\n",
        "    data['tweet_created'] = pd.to_datetime(data['tweet_date'])\n",
        "    return data\n",
        "\n",
        "\n",
        "data = load_data()\n",
        "\n",
        "st.sidebar.subheader(\"Show random tweets\")\n",
        "random_tweet = st.sidebar.radio('Select the Sentiment',('positive','negative','neutral'))\n",
        "\n",
        "st.subheader(\"Here are some example of tweets according to your choice!\")\n",
        "st.markdown(\"1.\" + data.query(\"labels == @random_tweet\")[['tweet']].sample(n=1).iat[0,0])\n",
        "\n",
        "\n",
        "st.sidebar.markdown(\"### Choose Model\")\n",
        "select = st.sidebar.selectbox('Model Type',['ANN-Sentiment Model','Naive Bayes Model'])\n",
        "\n",
        "st.sidebar.markdown(\"### Number of tweets\")\n",
        "select = st.sidebar.selectbox('Visualization Type',['BarGraph','PieChart'])\n",
        "\n",
        "sentiment_count = data['labels'].value_counts()\n",
        "sentiment_count = pd.DataFrame({'Sentiments':sentiment_count.index,'Tweets':sentiment_count.values})\n",
        "\n",
        "def choose_model(model):\n",
        "        st.markdown(\"### Choose Model\")\n",
        "        if select=='ANN-Sentiment Model':\n",
        "            text=np.array([tweet])\n",
        "            tk=Tokenizer(num_words=4000,lower=True,split=\" \")\n",
        "            tk.fit_on_texts(text)\n",
        "            prediction = model.predict_classes(pad_sequences(tk.texts_to_sequences(text),\n",
        "            maxlen=100))\n",
        "            if prediction == 0:\n",
        "              prediction = 'negative'\n",
        "              return prediction\n",
        "            elif prediction == 1:\n",
        "              prediction = 'neutral'\n",
        "              return prediction\n",
        "            else:\n",
        "              prediction = 'positive'\n",
        "              return prediction \n",
        "        else:\n",
        "            text=np.array([tweet])\n",
        "            tk=Tokenizer(num_words=4000,lower=True,split=\" \")\n",
        "            tk.fit_on_texts(text)\n",
        "            prediction2 = classifier.predict_classes(pad_sequences(tk.texts_to_sequences(text),\n",
        "            maxlen=100))\n",
        "            if prediction2 == 0:\n",
        "              prediction2 = 'negative'\n",
        "              return prediction2\n",
        "            elif prediction2 == 1:\n",
        "              prediction2 = 'neutral'\n",
        "              return prediction2\n",
        "            else:\n",
        "              prediction2 = 'positive'\n",
        "              return prediction2 \n",
        "\n",
        "if st.sidebar.checkbox('Show',False,key='0'):\n",
        "    st.markdown(\"### No. of tweets by sentiments \")\n",
        "    if select=='BarGraph':\n",
        "        fig = px.bar(sentiment_count,x='Sentiments',y='Tweets',color='Tweets',height=500)\n",
        "        st.plotly_chart(fig)\n",
        "    else:\n",
        "        fig = px.pie(sentiment_count,values='Tweets',names='Sentiments')\n",
        "        st.plotly_chart(fig)\n",
        "\n",
        "# Word cloud\n",
        "st.sidebar.subheader(\"Word Cloud\")\n",
        "word_sentiment = st.sidebar.radio(\"Which Sentiment to Display?\", tuple(pd.unique(data[\"labels\"])))\n",
        "if st.sidebar.checkbox(\"Show\", False, key=\"6\"):\n",
        "    st.subheader(f\"Word Cloud for {word_sentiment.capitalize()} Sentiment\")\n",
        "    df = data[data[\"labels\"]==word_sentiment]\n",
        "    words = \" \".join(data[\"tweet\"])\n",
        "    processed_words = \" \".join([word for word in words.split() if \"http\" not in word and not word.startswith(\"@\") and word != \"RT\"])\n",
        "    wordcloud = WordCloud(stopwords=STOPWORDS, background_color=\"white\", width=800, height=640).generate(processed_words)\n",
        "    plt.imshow(wordcloud)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    st.pyplot()\n",
        "\n",
        "if __name__=='__main__': \n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSPUMEHYwqng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ee513c8-3e50-400b-d40e-9a01fabd8bc4"
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-23 06:28:51--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 54.197.133.26, 3.223.239.191, 34.226.165.133, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|54.197.133.26|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13773305 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.13M  19.2MB/s    in 0.7s    \n",
            "\n",
            "2021-03-23 06:28:52 (19.2 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13773305/13773305]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6679q_6fwsJH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3261f2b-486e-42e4-f894-f7c4e109f34b"
      },
      "source": [
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGCM2OjvxNit"
      },
      "source": [
        "get_ipython().system_raw('./ngrok http 8501 &')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxwv00hdxedU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "939b8293-c96b-4c0e-ffba-8309f163d350"
      },
      "source": [
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    'import sys, json; print(\"Execute the next cell and the go to the following URL: \" +json.load(sys.stdin)[\"tunnels\"][0][\"public_url\"])'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Execute the next cell and the go to the following URL: https://a4d5094e7edc.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYC-XlBr0Psh",
        "outputId": "17ac586b-dced-46e9-d615-9319912ade52"
      },
      "source": [
        "!streamlit run /content/app.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.2:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.125.27.206:8501\u001b[0m\n",
            "\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning:\n",
            "\n",
            "sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "\n",
            "2021-03-23 06:36:09.252 NumExpr defaulting to 2 threads.\n",
            "2021-03-23 06:36:44.262567: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-03-23 06:36:45.453366: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-03-23 06:36:45.454355: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
            "2021-03-23 06:36:45.465996: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2021-03-23 06:36:45.466042: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (8df0c17413d8): /proc/driver/nvidia/version does not exist\n",
            "2021-03-23 06:36:45.466394: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-03-23 06:36:45.466582: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning:\n",
            "\n",
            "`model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "\n",
            "2021-03-23 06:36:45.594478: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
            "2021-03-23 06:36:45.604219: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 1999995000 Hz\n",
            "\u001b[34m  Stopping...\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}